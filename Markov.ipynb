{
 "metadata": {
  "name": "",
  "signature": "sha256:e537db86c40b89ae5e1d0e98248b2978477ec799b8542d7c2b65b7fed8a3c695"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import nltk \n",
      "from nltk.util import ngrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "test_sentence = 'The quick brown fox jumped over the lazy dog jumped over the speckled hen jumped over the lazy dog.'\n",
      "\n",
      "\n",
      "\n",
      "test_tok = tokenize_sentence(test_sentence)\n",
      "print test_tok\n",
      "assert test_tok[0] == ('^', '^')\n",
      "assert test_tok[1] == ('^', 'the')\n",
      "assert test_tok[2] == ('the', 'quick')\n",
      "assert test_tok[-2] == ('.', '^')\n",
      "assert test_tok[-1] == ('^', '^')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('^', '^'), ('^', 'the'), ('the', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumped'), ('jumped', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog'), ('dog', 'jumped'), ('jumped', 'over'), ('over', 'the'), ('the', 'speckled'), ('speckled', 'hen'), ('hen', 'jumped'), ('jumped', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog'), ('dog', '.'), ('.', '^'), ('^', '^')]\n"
       ]
      }
     ],
     "prompt_number": 125
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "probability_registry = defaultdict(nltk.FreqDist)\n",
      "\n",
      "def train_sentence(sentence, n=2):\n",
      "    \n",
      "    tokenized = tokenize_sentence(sentence, n)\n",
      "    \n",
      "    num_tokens = len(tokenized)\n",
      "    \n",
      "    print num_tokens\n",
      "    \n",
      "    for i, token in enumerate(tokenized):\n",
      "        next_token = tokenized[i + 1]\n",
      "        probability_registry[token][next_token[n - 1:]] += 1\n",
      "        \n",
      "        if i == num_tokens - 2:\n",
      "            break\n",
      "\n",
      "train_sentence(test_sentence, 2)\n",
      "\n",
      "print probability_registry\n",
      "\n",
      "prob_dist = nltk.UniformProbDist(probability_registry[('over', 'the')])\n",
      "\n",
      "for sample in prob_dist.samples():\n",
      "    print sample, prob_dist.prob(sample)\n",
      "    \n",
      "prob_dist = nltk.ELEProbDist(probability_registry[('over', 'the')])\n",
      "\n",
      "for sample in prob_dist.samples():\n",
      "    print sample, prob_dist.prob(sample)\n",
      "        \n",
      "prob_dist = nltk.MLEProbDist(probability_registry[('over', 'the')])\n",
      "for sample in prob_dist.samples():\n",
      "    print sample, prob_dist.prob(sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import nltk \n",
      "from nltk.util import ngrams\n",
      "\n",
      "class MarkovChain(object):\n",
      "    def __init__(self, n=2, divider_token='^'):\n",
      "        self.n = n\n",
      "        self.transition_probabilities = defaultdict(nltk.FreqDist)\n",
      "        self.divider_token = divider_token\n",
      "        \n",
      "    def tokenize_sentence(self, sentence):\n",
      "        sentence = sentence.lower()\n",
      "        sentence = (self.divider_token + ' ') * self.n + sentence + (' ' + self.divider_token) * self.n\n",
      "        word_tokenized = nltk.tokenize.word_tokenize(sentence)\n",
      "        return list(ngrams(word_tokenized,  self.n))\n",
      "    \n",
      "    def train_sentence(self, sentence):\n",
      "        tokenized = self.tokenize_sentence(sentence)\n",
      "        num_tokens = len(tokenized)\n",
      "        for i, token in enumerate(tokenized):\n",
      "            if i < num_tokens - 1:\n",
      "                next_token = tokenized[i + 1]\n",
      "                self.transition_probabilities[token][next_token[self.n - 1:]] += 1\n",
      "    \n",
      "    def _get_sentence_starter(self):\n",
      "        return self.n * (self.divider_token,)\n",
      "    \n",
      "    def get_next(self, current, weighted_by_probability=False):\n",
      "        \n",
      "        next_freq = self.transition_probabilities[current]\n",
      "        \n",
      "        if weighted_by_probability:\n",
      "            prob_dist = nltk.MLEProbDist(next_freq)\n",
      "            return prob_dist.generate()\n",
      "        else:\n",
      "            prob_dist = nltk.UniformProbDist(next_freq)\n",
      "            return prob_dist.generate()\n",
      "    \n",
      "    def generate_sentence(self, max_words=100):\n",
      "\n",
      "        sentence = current = start = self._get_sentence_starter()\n",
      "        count = 0\n",
      "        while True:\n",
      "            count += 1\n",
      "\n",
      "            next_token = self.get_next(current)\n",
      "            sentence = sentence + next_token\n",
      "            current = sentence[-self.n:]            \n",
      "            if current == start or count > max_words:\n",
      "                break\n",
      "        joined = ' '.join(sentence[self.n:-self.n -1])\n",
      "        joined = joined[0].upper() + joined[1:] + sentence[-self.n - 1]\n",
      "        return joined"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chain = MarkovChain()\n",
      "chain.train_sentence(test_sentence)\n",
      "chain.train_sentence(test_sentence)\n",
      "chain.train_sentence(test_sentence)\n",
      "chain.generate_sentence()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 177,
       "text": [
        "'The quick brown fox jumped over the lazy dog jumped over the lazy dog jumped over the lazy dog.'"
       ]
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foo = ('^', '^', 'the')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foo[-2:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "('^', 'the')"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}