{
 "metadata": {
  "name": "",
  "signature": "sha256:3f5cde7b8bb96c5969044983caf8f38b45ee32180184375b7e6796623aa32e50"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from bs4 import BeautifulSoup\n",
      "from unipath import Path\n",
      "import re\n",
      "\n",
      "nltk.download('punkt', '../data')\n",
      "nltk.data.path.append('../data/')\n",
      "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "\n",
      "UTF8_TAB = '\\xc2\\xa0'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = Path('../data')\n",
      "    \n",
      "prose = []\n",
      "english_riddles = []\n",
      "old_english_riddles = []\n",
      "for path in p.walk(filter=lambda p: p.isfile() and re.match(r\".*html\", p.name)):\n",
      "    soup = BeautifulSoup(path.read_file().decode('utf-8'))\n",
      "    prose = prose + [tag.text for tag in soup.select('div.prose p')]\n",
      "    for row in soup.select(\"table tr\"):\n",
      "        row_tds = row.findAll('td', {'align': 'left'})\n",
      "        if len(row_tds) > 1:\n",
      "            english_riddles.append(row_tds[0].text.strip().encode('utf-8'))\n",
      "            old_english_riddles = old_english_riddles + [tag.text.strip().encode('utf-8') for tag in row_tds[1:]]\n",
      "\n",
      "print len(prose)\n",
      "print len(english_riddles)\n",
      "print len(old_english_riddles)\n",
      "\n",
      "\n",
      "english_riddles[-1] = english_riddles[-1].replace(' (etc. as l. 2 above)', '') # Very specific fix\n",
      "\n",
      "riddle_sentences = []\n",
      "\n",
      "multiple_spaces = re.compile(ur'(\\s)+', re.UNICODE)\n",
      "space_before_punct = re.compile(ur' (\\W)', re.UNICODE)\n",
      "\n",
      "for riddle in english_riddles:\n",
      "    sentences = sent_detector.tokenize(riddle.strip().decode('utf-8'))\n",
      "    sentences = map(lambda x : x.encode('utf-8'), sentences)\n",
      "    only_real_sentences = filter(lambda x: len(x) >= 15, sentences)\n",
      "    cleaned = map(lambda x: re.sub(multiple_spaces, ' ', x.lower().replace('\\n', ' ').replace('\\xc2\\xa0', ' ')), only_real_sentences)\n",
      "    cleaned = map(lambda x: re.sub(space_before_punct, r'\\1', x), cleaned)\n",
      "\n",
      "    riddle_sentences = riddle_sentences + cleaned\n",
      "    \n",
      "# Clean stuff\n",
      "len(riddle_sentences)\n",
      "\n",
      "riddle_sentences[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, sentence in enumerate(riddle_sentences):\n",
      "    print \"%s :: %s (%s)\" % (i, sentence, len(sentence))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Markov!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import nltk \n",
      "from nltk.util import ngrams\n",
      "import random\n",
      "\n",
      "import re\n",
      "\n",
      "class MarkovChain(object):\n",
      "    def __init__(self, ngram_size=2, divider_token='^'):\n",
      "        self.n = ngram_size\n",
      "        self.transition_probabilities = defaultdict(nltk.FreqDist)\n",
      "        self.divider_token = divider_token\n",
      "        \n",
      "    def tokenize_sentence(self, sentence):\n",
      "        sentence = sentence.lower()\n",
      "        sentence = (self.divider_token + ' ') * self.n + sentence + (' ' + self.divider_token) * self.n\n",
      "        word_tokenized = nltk.tokenize.word_tokenize(sentence)\n",
      "        return list(ngrams(word_tokenized,  self.n))\n",
      "    \n",
      "    def train_sentence(self, sentence):\n",
      "        tokenized = self.tokenize_sentence(sentence.decode('utf-8'))\n",
      "        # Annoyting utf8 hijinx\n",
      "        tokenized = map(lambda x: tuple(map(lambda y: y.encode('utf-8'), x)), tokenized)\n",
      "        num_tokens = len(tokenized)\n",
      "        for i, token in enumerate(tokenized):\n",
      "            if i < num_tokens - 1:\n",
      "                next_token = tokenized[i + 1]\n",
      "                self.transition_probabilities[token][next_token[self.n - 1:]] += 1\n",
      "    \n",
      "    def _get_sentence_starter(self):\n",
      "        return self.n * (self.divider_token,)\n",
      "    \n",
      "    def get_next(self, current, weighted_by_probability=False):\n",
      "        \n",
      "        next_freq = self.transition_probabilities[current]\n",
      "        \n",
      "        if weighted_by_probability:\n",
      "            prob_dist = nltk.MLEProbDist(next_freq)\n",
      "            return prob_dist.generate()\n",
      "        else:\n",
      "            prob_dist = nltk.UniformProbDist(next_freq)\n",
      "            return prob_dist.generate()\n",
      "    \n",
      "    def _clean_sentence(self, generated_sentence):\n",
      "        cleaned = ' '.join(generated_sentence[self.n:-self.n])\n",
      "        cleaned = cleaned.capitalize()\n",
      "        cleaned = re.sub(' (\\W)', r'\\1', cleaned) # Remove space before punct\n",
      "        return cleaned\n",
      "    \n",
      "    def generate_sentence(self, max_words=100, weighted_by_probability=False):\n",
      "\n",
      "        sentence = current = start = self._get_sentence_starter()\n",
      "        count = 0\n",
      "        while True:\n",
      "            count += 1\n",
      "\n",
      "            next_token = self.get_next(current)\n",
      "            sentence = sentence + next_token\n",
      "            current = sentence[-self.n:]            \n",
      "            if current == start or count > max_words:\n",
      "                break\n",
      "        return self._clean_sentence(sentence)\n",
      "    \n",
      "class TweetGenerator(object):\n",
      "    \n",
      "    def __init__(self, riddle_sentences, ngram_size):\n",
      "        \n",
      "        self.riddle_sentences = set(riddle_sentences)\n",
      "        self.chain = MarkovChain(ngram_size=ngram_size)\n",
      "        \n",
      "        for sentence in self.riddle_sentences:\n",
      "            self.chain.train_sentence(sentence)\n",
      "    \n",
      "    def get_unique_sentence(self):\n",
      "        while True:\n",
      "            s = self.chain.generate_sentence()\n",
      "            if s.lower() not in self.riddle_sentences:\n",
      "                break\n",
      "        return s\n",
      "\n",
      "    def get_tweet(self):\n",
      "\n",
      "        tweet = ''\n",
      "\n",
      "        while True:\n",
      "            next_sentence = self.get_unique_sentence()\n",
      "            if len(next_sentence) > 120:\n",
      "                continue\n",
      "            if len(tweet) > 0 and len(tweet) + len(next_sentence) > 120:\n",
      "                break\n",
      "            tweet = tweet + next_sentence + ' '\n",
      "\n",
      "        return tweet[:-1].strip()\n",
      "    \n",
      "UTF8_TAB = '\\xc2\\xa0'\n",
      "\n",
      "def badassify_sentence(sentence):\n",
      "    new_sentence = ''\n",
      "    split_sentence = sentence.split(' ')\n",
      "    while len(split_sentence) > 0:\n",
      "        first_half = random.randrange(3, 5)\n",
      "        second_half = 4 if first_half == 3 else 3    \n",
      "        new_sentence = new_sentence + ' '.join(split_sentence[:first_half]) + 4 * UTF8_TAB + \\\n",
      "            ' '.join(split_sentence[first_half:first_half + second_half]) + '\\n' \n",
      "        split_sentence = split_sentence[first_half+second_half:]\n",
      "    return new_sentence.strip().strip(UTF8_TAB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tg = TweetGenerator(riddle_sentences, ngram_size=3)\n",
      "foo = tg.get_tweet()\n",
      "print foo\n",
      "print\n",
      "print badassify_sentence(foo)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "count = 100\n",
      "overage = 0\n",
      "\n",
      "generated = set()\n",
      "badass = set()\n",
      "tg = TweetGenerator(riddle_sentences, ngram_size=3)\n",
      "\n",
      "while count > 0:\n",
      "    tweet = tg.get_tweet()\n",
      "    badass_tweet =  badassify_sentence(tweet)\n",
      "    if len(badass_tweet) <= 140 and tweet not in generated:\n",
      "        generated.add(tweet)\n",
      "        badass.add(badass_tweet)\n",
      "        print badass_tweet\n",
      "        count -=1\n",
      "        if count % 100 == 0:\n",
      "            print count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}