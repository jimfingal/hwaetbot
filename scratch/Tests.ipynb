{
 "metadata": {
  "name": "",
  "signature": "sha256:3ceed8eabbe434da81169895151a20441a6cb979df98d5263add57fe0bc4e4f4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download('punkt', './data')\n",
      "nltk.data.path.append('./data/')\n",
      "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "from unipath import Path\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = Path('./riddlescrape/data/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "paths = []\n",
      "for html_path in p.walk(filter=lambda p: p.isfile() and re.match(r\".*html\", p.name)):\n",
      "    paths.append(html_path)\n",
      "\n",
      "paths"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prose = []\n",
      "english_riddles = []\n",
      "old_english_riddles = []\n",
      "for path in paths:\n",
      "    soup = BeautifulSoup(path.read_file())\n",
      "    prose = prose + [tag.text for tag in soup.select('div.prose p')]\n",
      "    for row in soup.select(\"table tr\"):\n",
      "        row_tds = row.findAll('td', {'align': 'left'})\n",
      "        if len(row_tds) > 1:\n",
      "            english_riddles.append(row_tds[0].text.strip())\n",
      "            old_english_riddles = old_english_riddles + [tag.text.strip() for tag in row_tds[1:]]\n",
      "print len(prose)\n",
      "print len(english_riddles)\n",
      "print len(old_english_riddles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, riddle in enumerate(english_riddles):\n",
      "    if len(riddle) > 0 and len(riddle) < 140:\n",
      "        print i, riddle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "riddle_sentences = []\n",
      "for riddle in english_riddles:\n",
      "    sentences = sent_detector.tokenize(riddle.strip())\n",
      "    riddle_sentences = riddle_sentences + filter(lambda x: len(x) > 15, sentences)\n",
      "len(riddle_sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i, sentence in enumerate(riddle_sentences):\n",
      "    print \"%s :: %s (%s)\" % (i, sentence, len(sentence))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r\"\"\"\n",
      "    NBAR:\n",
      "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
      "        \n",
      "    NP:\n",
      "        {<NBAR>}\n",
      "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
      "\"\"\"\n",
      "npchunker = nltk.RegexpParser(grammar)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_pos_tags(tokens):\n",
      "        return nltk.tag.pos_tag(tokens)\n",
      "\n",
      "def get_pos_tree(tokens):\n",
      "    postoks = get_pos_tags(tokens)\n",
      "    tree = npchunker.parse(postoks)\n",
      "    return tree\n",
      "\n",
      "def tokenize_words(txt):\n",
      "    return nltk.tokenize.word_tokenize(txt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_pos_tags(tokenize_words(riddle_sentences[2]))\n",
      "# JJ, NN, NNS "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from wordnik.swagger import ApiClient\n",
      "from wordnik.WordApi import WordApi\n",
      "apiUrl = 'http://api.wordnik.com/v4'\n",
      "apiKey = 'de028afe6c335a869300008d89e0abefda56bbb27e85579d0'\n",
      "client = ApiClient(apiKey, apiUrl)\n",
      "wordApi = WordApi(client)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "antonyms = wordApi.getRelatedWords(\"drives\", relationshipTypes=\"antonym\")\n",
      "context = wordApi.getRelatedWords(\"drives\", relationshipTypes=\"same-context\")\n",
      "\n",
      "related_words = set()\n",
      "if antonyms:\n",
      "    related_words.update([word.lower() for word in antonyms[0].words])\n",
      "if context:\n",
      "    related_words.update([word.lower() for word in context[0].words])\n",
      "related_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Markov!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import nltk \n",
      "from nltk.util import ngrams\n",
      "\n",
      "import re\n",
      "\n",
      "class MarkovChain(object):\n",
      "    def __init__(self, n=2, divider_token='^'):\n",
      "        self.n = n\n",
      "        self.transition_probabilities = defaultdict(nltk.FreqDist)\n",
      "        self.divider_token = divider_token\n",
      "        \n",
      "    def tokenize_sentence(self, sentence):\n",
      "        sentence = sentence.lower()\n",
      "        sentence = (self.divider_token + ' ') * self.n + sentence + (' ' + self.divider_token) * self.n\n",
      "        word_tokenized = nltk.tokenize.word_tokenize(sentence)\n",
      "        return list(ngrams(word_tokenized,  self.n))\n",
      "    \n",
      "    def train_sentence(self, sentence):\n",
      "        tokenized = self.tokenize_sentence(sentence)\n",
      "        num_tokens = len(tokenized)\n",
      "        for i, token in enumerate(tokenized):\n",
      "            if i < num_tokens - 1:\n",
      "                next_token = tokenized[i + 1]\n",
      "                self.transition_probabilities[token][next_token[self.n - 1:]] += 1\n",
      "    \n",
      "    def _get_sentence_starter(self):\n",
      "        return self.n * (self.divider_token,)\n",
      "    \n",
      "    def get_next(self, current, weighted_by_probability=False):\n",
      "        \n",
      "        next_freq = self.transition_probabilities[current]\n",
      "        \n",
      "        if weighted_by_probability:\n",
      "            prob_dist = nltk.MLEProbDist(next_freq)\n",
      "            return prob_dist.generate()\n",
      "        else:\n",
      "            prob_dist = nltk.UniformProbDist(next_freq)\n",
      "            return prob_dist.generate()\n",
      "    \n",
      "    def _clean_sentence(self, generated_sentence):\n",
      "        cleaned = ' '.join(generated_sentence[self.n:-self.n])\n",
      "        cleaned = cleaned.capitalize()\n",
      "        cleaned = re.sub(' (\\W)', r'\\1', cleaned) # Remove space before punct\n",
      "        return cleaned\n",
      "    \n",
      "    def generate_sentence(self, max_words=100):\n",
      "\n",
      "        sentence = current = start = self._get_sentence_starter()\n",
      "        count = 0\n",
      "        while True:\n",
      "            count += 1\n",
      "\n",
      "            next_token = self.get_next(current)\n",
      "            sentence = sentence + next_token\n",
      "            current = sentence[-self.n:]            \n",
      "            if current == start or count > max_words:\n",
      "                break\n",
      "        return self._clean_sentence(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chain = MarkovChain(n=3)\n",
      "for sentence in riddle_sentences:\n",
      "    chain.train_sentence(sentence)\n",
      "\n",
      "    \n",
      "\n",
      "def get_unique_sentence(chain, real_sentences):\n",
      "    s = chain.generate_sentence()\n",
      "    while s.lower() in real_sentences:\n",
      "        s = chain.generate_sentence()\n",
      "    return s\n",
      "\n",
      "\n",
      "def get_tweet(riddle_sentences, n=3):\n",
      "    chain = MarkovChain(n=n)\n",
      "    for sentence in riddle_sentences:\n",
      "        chain.train_sentence(sentence)\n",
      "\n",
      "    cleaned_riddles = map(lambda x: x.lower().encode('utf-8').replace('\\xa0', ''),\n",
      "                          riddle_sentences)\n",
      "    real_sentences = set()\n",
      "\n",
      "    tweet = ''\n",
      "\n",
      "    while True:\n",
      "        next_sentence = get_unique_sentence(chain, real_sentences)\n",
      "        if len(next_sentence) > 140:\n",
      "            continue\n",
      "        if len(tweet) > 0 and len(tweet) + len(next_sentence) > 139:\n",
      "            break\n",
      "        tweet = tweet + next_sentence + ' '\n",
      "\n",
      "    return tweet[:-1]\n",
      "get_tweet(riddle_sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_tweet(riddle_sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re.sub(' (\\W)', r'\\1', s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map(lambda x: x.lower(), riddle_sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import redis\n",
      "import os\n",
      "\n",
      "redis_url = os.getenv('HWAETBOT_REDISTOGO_URL', 'redis://localhost:6379')\n",
      "r = redis.from_url(redis_url)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(r.smembers('corpus'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from parse_riddles import parse_corpus\n",
      "\n",
      "sentences = set(parse_corpus())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sentence in sentences:\n",
      "    r.sadd('corpus', sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r.smembers('corpus')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r.delete('corpus')\n",
      "r.delete('used')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from twython import Twython\n",
      "import os\n",
      "\n",
      "consumer_key = os.environ.get('HWAETBOT_CONSUMER_KEY')\n",
      "consumer_secret = os.environ.get('HWAETBOT_CONSUMER_SECRET')\n",
      "access_token = os.environ.get('HWAETBOT_ACCESS_TOKEN')\n",
      "access_token_secret = os.environ.get('HWAETBOT_ACCESS_TOKEN_SECRET')\n",
      "\n",
      "\n",
      "twitter = Twython(consumer_key,\n",
      "                      consumer_secret,\n",
      "                      access_token,\n",
      "                      access_token_secret)\n",
      "\n",
      "twitter.get_application_rate_limit_status()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twitter.update_status(status=\"\"\"Swift was its moving,    faster than birds\n",
      "it flew through the    air and happily\n",
      "wafted to the shelter    of roofs.\"\"\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import redis\n",
      "import os\n",
      "\n",
      "r = redis.from_url('rediscloud')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "r.smembers('used')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from wordnik.swagger import ApiClient\n",
      "from wordnik.WordApi import WordApi\n",
      "apiUrl = 'http://api.wordnik.com/v4'\n",
      "apiKey = 'de028afe6c335a869300008d89e0abefda56bbb27e85579d0'\n",
      "client = ApiClient(apiKey, apiUrl)\n",
      "wordApi = WordApi(client)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "relateds = []                     \n",
      "                     \n",
      "words = [\"minority\", \"infancy\", \"cradle\", \"puberty\", \"bloom\"]\n",
      "\n",
      "for word in words:\n",
      "    related = wordApi.getRelatedWords(word)\n",
      "    relateds = relateds + related\n",
      "\n",
      "relateds"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = reduce(lambda x, y: x + y.words, relateds, [])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_words = reduce(lambda x, y: x + y.words, relateds, [])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "roget = requests.get(\"http://www.gutenberg.org/cache/epub/22/pg22.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thesaurus = roget.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matchword = re.compile(' (?P<name>n..se.y),')\n",
      "matchword.findall(thesaurus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matchword = re.compile(' (?P<name>m.....e),')\n",
      "matchword.findall(thesaurus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "print matchword.findall(thesaurus)\n",
      "\n",
      "for line in thesaurus:\n",
      "    print line\n",
      "    break\n",
      "    match = matchword.findall(line)\n",
      "    if match:\n",
      "        print match"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "hints = [\"thumb\", \"touch\", \"finger\", \"hand\", \"tact\", \"antenna\", \"forefinger\"]\n",
      "\n",
      "matchword = re.compile(' (?P<name>performer)', flags=re.IGNORECASE)\n",
      "\n",
      "c = Counter()\n",
      "for i, line in enumerate(thesaurus.split('#')):\n",
      "    match = matchword.findall(line)\n",
      "    if match:\n",
      "        print line\n",
      "        print \"*\" * 80\n",
      "print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}